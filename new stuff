
- andrew ng "Better Than Backprop

End-to-end backpropagation and labeled data are the peanut butter and chocolate of deep learning. 
However, recent work suggests that neither is necessary to train effective neural networks to represent complex data. 
What’s new: Sindy Löwe, Peter O’Connor, and Bastiaan Veeling propose Greedy InfoMax (GIM),
an unsupervised method for learning to extract features that trains only one layer at a time.

Key insight: The information bottleneck theory (IB) suggests that neural networks work by
concentrating information like a data-compression algorithm. In data compression,
the amount of information retained is measured in mutual information (MI) between original and compressed versions. 
IB says that neural nets maximize MI between each layer’s input and output. Thus GIM reframes learning
as a self-supervised compression problem. Unlike earlier MI-based approaches, it optimizes each layer separately.


    GIM uses the previous layer’s output as the next layer’s input to train each layer independently. 
    This differs from the usual backpropagation in which all layers learn at once.
    
    The researchers devised a task that teaches layers to extract features that maximize MI.
    Given a subsequence of input data that has been compressed according to the current weights, 
    the layer predicts the next element in the compressed sequence, choosing from a random selection drawn 
    from the input including the correct choice. High success demonstrates that the layer is able to compress the input.
    
    The process effectively removes redundancy between nearby regions of the input. 
    For example, a recording of a song’s chorus may repeat several times, 
    so it’s possible to represent the recording without capturing the repetitions.
    
    
    --- MLPerf
    
        Fair and useful benchmarks for measuring -training and inference- performance of ML hardware, software, and services.

 
 

----
"" New software can now fool the AI behind Alexa and Siri.

Software called TextFooler can now trick natural-language processing (NLP) systems into misunderstanding text 
just by replacing certain words in a sentence with synonyms. 
Developed by a team at MIT, it looks for the words that are most important to an NLP classifier
and replaces them with a synonym that a human would find natural. 
For example, it changes the sentence “The characters, cast in impossibly contrived situations, 
are totally estranged from reality” to “The characters, cast in impossibly engineered circumstances,
are fully estranged from reality.”
We see no difference, but AI interprets the sentences completely differently.


-----
"" Medication madness. According to the FDA, serious adverse drug interactions are estimated to kill 
more than 100,000 hospitalized people in the US every year.
But avoiding such interactions during drug development is laborious and expensive. 
It involves intensive testing and clinical trials to catalogue all the proposed drug’s possible chemical interactions
with existing ones.

Now a new AI system could make this easier by predicting the interactions
between two drugs based only on their chemical structure. 
The researchers first developed a new way to represent the 3D chemical structures of drugs in a character format
that could be read by a neural network. The drug melatonin, for example, is represented by “CC(=O)NCCC1=CNc2c1cc(OC)cc2.”

They then translated a database of known drug interactions into this format
and used it to train a neural network. The resulting system predicts the probability 
that two drugs will have an adverse interaction and shows the particular parts of the molecule 
that contributed to that prediction. When the researchers tested their system on two common drug interaction data sets,
it performed better than state-of-the-art results. Read more here.


---" Quotable

It’s really snowballed.

—Vincent Cate, who lives on the island of Anguilla, 
which has become an unlikely benefactor of the AI boom because of its domain name “.ai”


---Double descent ---
occurs when a model’s performance changes in unpredictable ways
as the amount of training data or number of parameters crosses a certain threshold. 
The error falls as expected with additional data or parameters, but then rises, drops again, and may take further turns. 
Preetum Nakkiran and
collaborators at Harvard, Stanford, and Microsoft found a way to eliminate double descent in some circumstances.


-
Probabilistic Programming Languages

https://towardsdatascience.com/a-gentle-introduction-to-probabilistic-programming-languages-ba9105d9cbce

"Conceptually, probabilistic programming languages(PPLs) are domain-specific languages
that describe probabilistic models 
and the mechanics to perform inference in those models.
The magic of PPL relies on combining the inference capabilities of probabilistic methods 
with the representational power of programming languages.





--
One-shot learning

https://towardsdatascience.com/one-shot-learning-with-siamese-networks-using-keras-17f34e75bb3d

 There are applications wherein we neither have enough data for each class and the total number classes is huge 
 as well as dynamically changing. 
 Thus, the cost of data collection and periodical re-training is -->  too high.
 
 


--
Siamese networks
https://towardsdatascience.com/one-shot-learning-with-siamese-networks-using-keras-17f34e75bb3d

Typically the similarity score is squished between 0 and 1 using a sigmoid function;
wherein 0 denotes no similarity and 1 denotes full similarity. 
Any number between 0 and 1 is interpreted accordingly.

Notice that this network is not learning to classify an image directly to any of the output classes.
Rather, it is learning a similarity function,
which takes two images as input and expresses how similar they are.

Oneshot
But the biggest advantage is that , let’s say in case of face recognition,
we have a new employee who has joined the organization. Now in order for the network to detect his face, 
we only require a single image of his face which will be stored in the database.
Using this as the reference image,
the network will calculate the similarity for any new instance presented to it.
Thus we say that network predicts the score in one shot.

Notice that we will train the system on one set of characters 
and then test it on a completely different set of characters
which were never used during the training. 
This is not possible in a traditional classification cycle.

Mapping the problem to binary classification task
Let’s understand how can we map this problem into a supervised learning task 
where our dataset contains pairs of (Xi, Yi) 
where ‘Xi’ is the input and ‘Yi’ is the output.

Recall that the input to our system will be a pair of images
and the output will be a similarity score between 0 and 1.

Xi = Pair of images

Yi = 1 ; if both images contain the same character

Yi = 0; if both images contain different characters

Thus we need to create pairs of images along with the target variable, as shown above, 
to be fed as input to the Siamese Network.
Note that even though characters from Sanskrit alphabet are shown above, 
but in practice we will generate pairs randomly from all the alphabets in the training data.

Intuition: The term Siamese means twins. 
The two Convolutional Neural Networks shown above are not different networks
but are two copies of the same network, hence the name Siamese Networks.
Basically they share the same parameters. 
The two input images (x1 and x2) are passed through the ConvNet 
to generate a fixed length feature vector for each (h(x1) 
and h(x2)). 
Assuming the neural network model is trained properly, we can make the following hypothesis: 
If the two input images belong to the same character, then their >>feature vectors must also be similar,
while if the two input images belong to the different characters, then their feature vectors will also be different.
Thus the element-wise absolute difference 
between the two feature vectors 
must be very different in both the above cases. 
And hence the similarity score generated by the output sigmoid layer must also be different in these two cases.
This is the central idea behind the Siamese Networks.

Notice that there is no predefined layer in Keras to compute the absolute difference between two tensors.
We do this using the Lambda layer in Keras which is used to add customized layers in Keras.
    # Add a customized layer to compute the absolute difference between the encodings
    L1_layer = Lambda(lambda tensors:K.abs(tensors[0] - tensors[1]))
    L1_distance = L1_layer([encoded_l, encoded_r])


The model was compiled using the adam optimizer and binary cross entropy loss function.
Learning rate was kept low as it was found that with high learning rate, the model took a lot of time to converge. 
However these parameters can well be tuned further to improve the present settings.

The model was trained for 20,000 iterations with batch size of 32.
After every 200 iterations, model validation was done using 20-way one shot learning 
and the accuracy was calculated over 250 trials.

VAlidate and test
Note that, for every pair of input images, 
our model generates a similarity score between 0 and 1. 
But just looking at the score its difficult to ascertain whether the model is really able to recognize
similar characters and distinguish dissimilar ones.
A nice way to judge the model is >>>>> N-way one shot learning.

Basically the same character is compared to 4 different characters 
out of which only one of them matches the original character. 
Let’s say by doing the above 4 comparisons we get 4 similarity scores S1, S2, S3 and S4 as shown.
Now if the model is trained properly, we expect that S1 is the maximum of all the 4 similarity scores
because the first pair of images is the only one where we have two same characters.

Thus if S1 happens to be the maximum score,
we treat this as a correct prediction otherwise we consider this as an incorrect prediction.
Repeating this procedure ‘k’ times, we can calculate the percentage of correct predictions as follows.


Base Line 1 — Nearest Neighbor Model
It is always a good practice to create simple baseline models and compare their results with the complex model 
you are trying to build.

Conclusion
This is just a first cut solution and many of the hyper parameters can be tuned >>in order to avoid over fitting.
Also more rigorous testing can be done by increasing the value of ’N’ 
in N-way testing and by increasing the number of trials.

--
tf.keras.layers.Lambda
Wraps arbitrary expressions as a Layer object.

Inherits From: Layer


tf.keras.layers.Lambda(
    function, output_shape=None, mask=None, arguments=None, **kwargs)
    
    in siamese 
    # Add a customized layer to compute the absolute difference between the encodings
    L1_layer = Lambda(lambda tensors:K.abs(tensors[0] - tensors[1]))
    L1_distance = L1_layer([encoded_l, encoded_r])
    
    
    --
    Lambda functions
    
    Python and other languages like Java, C#, and even C++ have had lambda functions added to their syntax,
    whereas languages like LISP or the ML family of languages, Haskell, OCaml, and F#, use lambdas as a core concept.

>>Python lambdas are little, anonymous functions, 
subject to a more restrictive but more concise syntax than regular Python functions.

By the end of this article, you’ll know:

    How Python lambdas came to be
    How lambdas compare with regular function objects
    How to write lambda functions
    Which functions in the Python standard library leverage lambdas
    When to use or avoid Python lambda functions
    
    History

Alonzo Church formalized lambda calculus,
a language based on pure abstraction, in the 1930s. 
Lambda functions are also referred to as lambda abstractions, 
a direct reference to the abstraction model of Alonzo Church’s original creation.

Lambda calculus can encode any computation. 
It is Turing complete, but contrary to the concept of a Turing machine, it is pure and >>does not keep any state.

Functional languages
get their origin in mathematical logic and lambda calculus,
while imperative programming languages
embrace the state-based model of computation invented by Alan Turing.
The two models of computation, lambda calculus and Turing machines, can be translated into each another.
This equivalence is known as the Church-Turing hypothesis.

Functional languages directly inherit the lambda calculus philosophy,
adopting a declarative approach of programming that emphasizes abstraction,
data transformation, composition, and purity (no state and no side effects).
Examples of functional languages include Haskell, Lisp, or Erlang.

By contrast, the Turing Machine led to imperative programming found in languages like Fortran, C, or Python.

The imperative style consists of programming with statements,
driving the flow of the program step by step with detailed instructions.
This approach promotes mutation and requires managing state.

Python is not inherently a functional language,
but it adopted some functional concepts early on. 
In January 1994, map(), filter(), reduce(), and the lambda operator were added to the language.

The following terms may be used interchangeably depending on the programming language type and culture:

    Anonymous functions
    Lambda functions
    Lambda expressions
    Lambda abstractions
    Lambda form
    Function literals

For the rest of this article after this section, you’ll mostly see the term lambda function.

Taken literally, an anonymous function is a function without a name.
In Python, an anonymous function is created with the lambda keyword. 
More loosely, it may or not be assigned a name.
Consider a two-argument anonymous function defined with lambda but not bound to a variable.
The lambda is not given a name:

>>> lambda x, y: x + y

The function above defines a lambda expression that takes two arguments and returns their sum.
--

Deep Double Descent

OpenAI

https://openai.com/blog/deep-double-descent/

We show that the double descent phenomenon occurs in CNNs, ResNets, and transformers: performance first improves,
then gets worse, and then improves again with increasing model size, data size, or training time. 
This effect is often avoided through careful regularization.
While this behavior appears to be fairly universal, we don’t yet fully understand why it happens,
and view further study of this phenomenon as an important research direction.

Moreover, we show that double descent occurs not just as a function of model size,
but also as a function of the number of training epochs. 

We unify the above phenomena by defining a new complexity measure 
we call the effective model complexity 
and conjecture a generalized double descent with respect to this measure. 

Furthermore, our notion of model complexity allows us to identify certain regimes 
where increasing (even quadrupling) the number of train samples 
actually hurts test performance.



---

Life long ML
Chen and Lei

Lifelong Machine Learning or Lifelong Learning (LL) is an advanced machine learning (ML) paradigm 
that learns continuously, accumulates the knowledge learned in the past,
and uses/adapts it to help future learning and problem solving. 
In the process, the learner becomes more and more knowledgeable and better and better at learning.



__
Semi supervised learning

FixMatch: Simplifying Semi-Supervised Learning with Consistency and Confidence

Kihyuk Sohn, David Berthelot, Chun-Liang Li, Zizhao Zhang, Nicholas Carlini, Ekin D. Cubuk, Alex Kurakin, Han Zhang, Colin Raffel

    Semi-supervised learning (SSL) provides an effective means of leveraging unlabeled data
    to improve a model's performance.
    In this paper, we demonstrate the power of a simple combination of two common SSL methods:
    consistency regularization and pseudo-labeling.
    
    Our algorithm, FixMatch, first generates pseudo-labels
    using the model's predictions on weakly-augmented unlabeled images.
    
    >>For a given image, the pseudo-label is only retained if the model produces a high-confidence prediction.
    The model is then trained to predict the pseudo-label when fed a strongly-augmented version of the same image.
    Despite its simplicity, we show that FixMatch achieves state-of-the-art performance 
    across a variety of standard semi-supervised learning benchmarks,
    including 94.93% accuracy on CIFAR-10 with 250 labels and 88.61% accuracy with 40 -- just 4 labels per class.
    
    Since FixMatch bears many similarities to existing SSL methods 
    that achieve worse performance, we carry out an extensive ablation study
    to tease apart the experimental factors that are most important to FixMatch's success.
    We make our code available at this https URL. 



---

Q Learning


Q-learning is an off policy reinforcement learning algorithm 
that seeks to find the best action to take given the current state. 

It’s considered off-policy because the q-learning function learns from actions that are outside the current policy,
like taking random actions, and therefore a policy isn’t needed. 

More specifically, q-learning seeks to learn a policy that maximizes the total reward.

The ‘q’ in q-learning stands for quality. 
Quality in this case represents how useful a given action is in gaining some future reward.

Create a q-table
When q-learning is performed we create what’s called a q-table or matrix 
that follows the shape of [state, action] and we initialize our values to zero.
We then update and store our q-values after an episode. 
This q-table becomes a reference table for our agent to select the best action based on the q-value.




--
Graph Deep Learning (GDL) 
Graph Neural Network (GNN)
https://medium.com/dair-ai/an-illustrated-guide-to-graph-neural-networks-d5564a551783

The black arrows on the edges represent the kind of relationship between the nodes.
It shows whether a relationship is mutual or one-sided.
The two different kinds of graphs are 
---directed (connection direction matters between nodes)
----- undirected (connection order doesn’t matter). 
Directed graphs can be unidirectional or bidirectional in nature.

A graph can represent many things — social media networks, molecules, etc.
Nodes can be thought of as users/products/atoms 
while the edges represent connections (following/usually-purchased-with/bonds). 
A social media graph may look like this with nodes as users and edges as connections

Each node has a set of features defining it. 
In the case of social network graphs, this could be age, gender, country of residence, political leaning, and so on. 
Each edge may connect nodes together that have similar features. (yo/ necessarily similar? nah, otras relaciones tmb)
It shows some kind of interaction or relationship between them.

Message Passing
Once the conversion of nodes and edges are completed,
the graph performs Message Passing between the nodes. 
This process is also called Neighbourhood Aggregation 
because it involves pushing messages (aka, the embeddings) 
from surrounding nodes around a given reference node, through the directed edges. (feed forward nns)

Once you perform the Neighbourhood Aggregation/Message Passing procedure a few times,
you obtain a completely new set of embeddings for each nodal recurrent unit.

Through the timesteps/rounds of Message Passing, 
the nodes know more about their own information (features) and that of neighbouring nodes. 
This creates an even more accurate representation of the entire graph.

For further processing in higher layers of a pipeline, or simply to represent the graph,
you can take all the embeddings and sum them up together
to get vector H that represents the whole graph.

((To summarise this step, we sum together the final vector representations
of all nodal recurrent units (order-invariant,of course)
use this resulting vector as inputs to other pipelines
or to simply represent the graph.))

GNNs are fairly simple to use. In fact, implementing them involved four steps.

    1.Given a graph, we first convert the nodes to recurrent units
    and the edges to feed-forward neural networks.
    2.Then we perform Neighbourhood Aggregation (Message Passing, if that sounds better) 
    for all nodes n number of times.
    3.Then we sum over the embedding vectors of all nodes 
    to get graph representation H.
    4.Feel free to pass H into higher layers
    or use it to represent the graph’s unique properties!

Why Graph Neural Networks?
Now that we know how Graph Neural Networks work, why would we want to apply/use them?
In the case of social media graphs, GNNs are great at content recommendation. 
When a user follows other users with a similar taste in political leaning (for example), 
GNNs can be used for node classification
to predict if a certain piece of content on the site 
can be sent to the news feed of said user.

When suggesting “who to follow”, systems can take into account the industry of the user 
and provide potential connections — edge classification.

There are also great resources to learn about GDL algorithms 
and different ways to capture lots of sequential and spatial aspects from graph representations.

https://github.com/rusty1s/pytorch_geometric
https://pytorch-geometric.readthedocs.io/
https://www.dgl.ai/
http://geometricdeeplearning.com/

Geometric Learning
In the last decade, Deep Learning approaches (e.g. Convolutional Neural Networks and Recurrent Neural Networks)
allowed to achieve unprecedented performance on a broad range of problems
coming from a variety of different fields (e.g. Computer Vision and Speech Recognition).

Despite the results obtained, 
research on DL techniques has mainly focused so far on data defined on Euclidean domains (i.e. grids).

Nonetheless, in a multitude of different fields, such as: Biology, Physics, Network Science,
Recommender Systems and Computer Graphics; 
one may have to deal with data defined on non-Euclidean domains (i.e. graphs and manifolds).

The adoption of Deep Learning in these particular fields has been lagging behind until very recently,
primarily since the non-Euclidean nature of data 
makes the definition of basic operations (such as convolution) rather elusive.

Geometric Deep Learning deals in this sense 
with the extension of Deep Learning techniques to graph/manifold structured data.


--
A Theory of Fermat Paths for Non-Line-of-Sight Shape Reconstruction

We present a novel theory of Fermat paths of light
between a known visible scene and an unknown object not in the line of sight of a transient camera. 

These light paths either obey specular reflection or are reflected by the object’s boundary,
and hence encode the shape of the hidden object.

We prove that Fermat paths correspond to discontinuitiesin the transient measurements. 
We then derive a novel constraint 
that relates the spatial derivatives of the path lengths at these discontinuities to the surface normal.

Based on this theory, we present an algorithm, called Fermat Flow, 
to es-timate the shape of the non-line-of-sight object.

Our method allows, for the first time, accurate shape recovery of com-plex objects,
ranging from diffuse to specular, that are hid-den around the corner as well as hidden behind a diffuser.




--

Pruning 

Michela Paganini, Postdoctoral Researcher at Facebook AI, 
shares her personal experience creating a core PyTorch feature: Pruning (torch.nn.utils.prune). 

State-of-the-art deep learning techniques rely on over-parametrized models that are hard to deploy. 
On the contrary, biological neural networks are known to use efficient sparse connectivity. 

Identifying optimal techniques to compress models by reducing the number of parameters in them is important
in order to reduce memory, battery, and hardware consumption without sacrificing accuracy, 
deploy lightweight models on device, and guarantee privacy with private on-device computation. 

On the research front, pruning is used to investigate the differences in learning dynamics between over-parametrized and under-parametrized networks, 
to study the role of lucky sparse subnetworks and initializations (“lottery tickets”) as a destructive neural architecture search technique, and more.




--
https://gizmodo.com/this-algorithm-might-make-facial-recognition-obsolete-1844591686

 By swapping out or distorting some of these pixels,
 the face might still be recognizable to you or me, 
 but it would register as an entirely different person to just about every popular facial recognition algo. 
 
 According to the team’s research, this “cloaking” technique managed to fool the facial recognition systems
 peddled by Microsoft, Amazon, and Google 100% of the time.

If you want to give this algo a whirl yourself, the good news is that the U. Chicago team has the Fawkes program 
freely available for download on their website. If you have a picture you want to protect from snoopers or scrapers, 
you can load them into Fawkes, which then jumbles those unseen pixels in about 40 seconds per photograph, according to the researchers. 


---

A critical analysis of self-supervision, or what we can learn from a single image
Yuki M. Asano, Christian Rupprecht, Andrea Vedaldi

    We look critically at popular self-supervision techniques for learning deep convolutional neural networks without manual labels.
    We show that three different and representative methods, BiGAN, RotNet and DeepCluster,
    can learn the first few layers of a convolutional network from a single image 
    as well as using millions of images and manual labels, provided that strong data augmentation is used. 
    
    However, for deeper layers the gap with manual supervision cannot be closed even if millions of unlabelled images are used for training.





----
Efficient Nets


Image Classification with EfficientNet: -----> Better performance with computational efficiency
Anand Borad
https://medium.com/analytics-vidhya/image-classification-with-efficientnet-better-performance-with-computational-efficiency-f480fdb00ac6

In May 2019, two engineers from Google brain team named Mingxing Tan and Quoc V. Le 
published a paper called “EfficientNet: Rethinking Model Scaling for Convolutional Neural Networks”. 
The core idea of publication was about strategically scaling deep neural networks 
but it also introduced a new family of neural nets, EfficientNets.

EfficientNets, as the name suggests are very much efficient computationally
and also achieved state of art result on ImageNet dataset which is 84.4% top-1 accuracy.

So, in this article, we will discuss EfficientNets in detail but first, we will talk about the core idea introduced in the paper, 
-->model scaling.

Model scaling is about scaling the existing model in terms of model depth, model width, and less popular input image resolution
to improve the performance of the model. 
Depth wise scaling is most popular amongst all, e.g. ResNet can be scaled from Resnet18 to ResNet200. 
Here ResNet10 has 18 residual blocks and can be scaled for depth to have 200 residual blocks.

ResNet200 delivers better performance than ResNet18 and thus, manually scaling works pretty well. 
But there is one problem with traditional manual scaling method, after a certain level, scaling doesn’t improve performance.
It starts to affect adversely by degrading performance.

The scaling method introduced in paper is named -->compound scaling 
and suggests that instead of scaling only one model attribute out of depth, width, and resolution; 
-->strategically scaling all three of them together delivers better results.

Compound scaling
--->Compound scaling method uses a compound co-efficient ø to scale width, depth, and resolution together.




----

Cut, Paste and Learn: Surprisingly Easy Synthesis for Instance Detection
Debidatta Dwibedi, Ishan Misra, Martial Hebert
https://arxiv.org/abs/1708.01642

    A major impediment in rapidly deploying object detection models for instance detection
    is the lack of large annotated datasets.
    
    For example, finding a large labeled dataset containing instances in a particular kitchen is unlikely.
    Each new environment with new instances requires expensive data collection and annotation. 
    
    In this paper, 
    >>we propose a simple approach to generate large annotated instance datasets with minimal effort. 
    
    Our key insight is that ensuring only patch-level realism 
    provides enough training signal for current object detector models. 
    We automatically `cut' object instances and `paste' them on random backgrounds. 
    
    A naive way to do this results in pixel artifacts which result in poor performance for trained models.
    We show how to make detectors ignore these artifacts during training 
    and generate data that gives competitive performance on real data. 
    
    Our method outperforms existing synthesis approaches
    and when combined with real images 
    improves relative performance by more than 21% on benchmark datasets.
    
    In a cross-domain setting, our synthetic data combined with just 10% real data 
    outperforms models trained on all real data. 


-----------

Frustratingly Simple Few-Shot Object Detection

https://arxiv.org/pdf/2003.06957.pdf

Abstract
Detecting rare objects from a few examples is an emerging problem.  
Prior works show meta-learning  is  a  promising  approach.  
But,  fine-tuning techniques have drawn scant attention. 

We find  that  fine-tuning  only  the  last  layer  of  existing detectors on rare classes is crucial 
to the few-shot object detection task.   

Such a simple approach  outperforms  the  meta-learning  methods
by roughly 2∼20 points on current benchmarks and sometimes even doubles the accurac yof the prior methods.  

However,  the high variance in the few samples 
often leads to the unreliability  of  existing  benchmarks.

We  revise the  evaluation  protocols  
by  sampling  multiple groups of training examples
to obtain stable comparisons  
and  build  new  benchmarks  based  on three datasets: PASCAL VOC, COCO and LVIS.

Again, our fine-tuning approach establishes a new state of the art on the revised benchmarks.
The code as well as the pretrained models are available at https://github.com/ucbdrive/few-shot-object-detection.


--------

One-bit Supervision for Image Classification
https://arxiv.org/pdf/2009.06168.pdf

Abstract
This paper presents one-bit supervision, a novel setting of learning from incomplete annotations, in the scenario of image classification.
Instead of training a model upon the accurate label of each sample, 
our setting requires the model to query with a predicted label of each sample 
and learn from the answer whether the guess is correct. 

This provides one bit (yes or no) of information, and more importantly,
annotating each sample becomes much easier than finding the accurate label from many candidate classes.  

There are two keys to training a model upon one-bit supervision: 
improving the guess accuracy 
and making use of incorrect guesses.

For these purposes, we propose a multi-stage training paradigm 
which incorporates negative label suppression into an off-the-shelf semi-supervised learning algorithm. 
In three popular image classification benchmarks, our approach claims higher efficiency in utilizing the limited amount of annotations.


------

Relational inductive biases, deep learning, and graph networks   (combinatorial generalization)

https://arxiv.org/pdf/1806.01261.pdf

The  following  is  part  position  paper,  part  review,  and  part  unification. 
We  argue  that combinatorial generalization 
must be a top priority for AI to achieve human-like abilities,
and that structured representations and computations are key to realizing this objective. 

Just as biology uses nature and nurture cooperatively,
we reject the false choice between “hand-engineering”and “end-to-end” learning, 
and instead advocate for an approach which benefits from their complementary strengths. 

We explore how using relational inductive biases 
within deep learning architectures 
can facilitate learning about entities, relations, and rules for composing them.  

We present a new building block for the AI toolkit with a strong relational inductive bias
—the graph network—
which generalizes and extends various approaches for neural networks that operate on graphs,
and provides a straightforward interface 
for manipulating structured knowledge and producing structured behaviors.  

We discuss how graph networks can support relational reasoning and combinatorial generalization, 
laying the foundation for more sophisticated, interpretable,and flexible patterns of reasoning.  

As a companion to this paper,  we have also released anopen-source software library for building graph networks, 
with demonstrations of how to use them in practice.




---------------------

DNA computing

https://interestingengineering.com/what-is-dna-computing-how-does-it-work-and-why-its-such-a-big-deal

The reason this generated excitement was that DNA structures are cheap, relatively easy to produce, and scalable. 
There is no limit to the power that DNA computing can theoretically have 
since its power increases the more molecules you add to the equation 
and unlike silicon transistors which can perform a single logical operation at a time,
these DNA structures can theoretically perform as many calculations at a time as needed to solve a problem 
and do it all at once.

The problem however, is speed.
Even though it took moments for Adleman’s solution to the traveling salesman problem to be encoded into his DNA strands in the test tube,
it took days of filtering out bad solutions to find the optimal solution he was looking for
—after meticulous preparation for this single computation.

Still, the concept was a sound one and the potential for incredible gains 
in storage capacity
and computational speeds was obvious.
This kicked off two decades of research into how to create practical DNA computing a reality.


As demonstrated with Adleman’s paper (1994), 
the major advantage of DNA computing over classical computing—and even quantum computing to an extent—
>>>>>> is that it can perform countless calculations in parallel. 
This idea of parallel computing isn’t new and has been mimicked in classical computing for decades.


https://www.wired.com/story/finally-a-dna-computer-that-can-actually-be-reprogrammed/

But there’s a problem: The molecular circuits built so far have no flexibility at all. 
Today, using DNA to compute is “like having to build a new computer out of new hardware just to run a new piece of software,”
says computer scientist David Doty. So Doty, a professor at UC Davis, and his colleagues set out to see
what it would take to implement a DNA computer that was in fact 
> reprogrammable.

They showed it’s possible to use a simple trigger
to coax the same basic set of DNA molecules 
into implementing numerous different algorithms.

“There was >> algorithmic self-assembly before, 
but not to this degree of complexity.”

In electronic computers like the one you’re using to read this article, 
bits are the binary units of information that tell a computer what to do. 
They represent the discrete physical state of the underlying hardware, 
usually the presence or absence of an electrical current. 

These bits, or rather the electrical signals implementing them,
are passed through circuits made up of logic gates, 
which perform an operation on one or more input bits 
and produce one bit as an output.

By combining these simple building blocks over and over, 
computers are able to run remarkably sophisticated programs.
The idea behind DNA computing is to substitute 
chemical bonds
for electrical signals 
and nucleic acids 
for silicon 
to create biomolecular software. 

According to Erik Winfree, a computer scientist at Caltech and a co-author of the paper,
molecular algorithms 
leverage the natural information processing capacity baked into DNA,
but rather than letting nature take the reins, he says,
“computation controls the growth process.”


-------
6.2.2 Self-Training (Bootstrapping)

Mahendra and Schmdi

Self-training, sometimes also referred to as bootstrapping or pseudo-labels, 
is an iterative method where a deep neural network is first developed in a supervised fashion on the labelled data. 

This neural network is then used to provide (pseudo) labels to the unlabelled data, 
which can then be used in conjunction with the labelled data to train a new, more accurate neural network. 

This approach often works well and can even be repeated to get further improvements. 
There are a couple of common details in implementation — often when adding the neural network pseudo-labelled data, 
we only keep the most confidently pseudo-labelled examples. 

These pseudo-labelled examples may also be used for 
training with a different objective function 
compared to the labelled data. 

Other variants, including mean teacher [225],
temporal ensembling [119]
and the recent MixMatch [19]
also primarily use the self-training approach, but incorporate elements of consistency (see below).

There are nice open sourced implementations of these methods,
such as https://github.com/CuriousAI/mean-teacher for mean teacher 
and https://github.com/google- research/mixmatch and
https://github.com/YU1ut/MixMatch-pytorch for MixMatch.


6.2.3 Enforcing Consistency (Smoothness)

An important theme in many semi-supervised methods has been to
provide supervision on the unlabelled data 
through enforcing consistency. 

If a human was given two images A and B, 
where B was a slightly perturbed version of A (maybe blurred, maybe some pixels obscured or blacked out), 
they would give these images the same label — consistency. 

We can also apply this principle to >>> provide feedback to our neural network on the unlabelled data,
combining it with the labelled data predictions as in multitask learning (Section 5.3)
to form a semi-supervised learning algorithm. 

A popular method on enforcing consistency is virtual adversarial training [155],
which enforces consistency across carefully chosen image perturbations.

Another paper, unsupervised data augmentation [251], 
uses standard data augmentation techniques such as cutout [44]
for images and back translation for text [206] 
to perturb images and enforces consistency across them. 

[265] uses consistency constraints 
along with other semi-supervised and self-supervised techniques in its full algorithm.


------------

Learning Not to Learn: Training Deep Neural Networks with Biased Data

Byungju Kim1Hyunwoo Kim2Kyungsu Kim3Sungjin Kim3Junmo Kim1

Abstract
We  propose  a  novel  regularization  algorithm  to  train deep  neural  networks,  
in  which  data  at  training  time  isseverely biased.  

Since a neural network efficiently learns data distribution,  
a network is likely to learn the bias information to categorize input data.  
It leads to poor perfor-mance at test time, if the bias is, in fact, irrelevant to the categorization.

In this paper, we formulate a regularization loss based on mutual information between feature embedding and bias.

Based on the idea of minimizing this mutual information, 
we propose an iterative algorithm to unlearn the bias information.  

We employ an additional network to predict the bias distribution 
and train the network adversarially against the feature embedding network. 

At the end oflearning, the bias prediction network is not able to predict the bias not because it is poorly trained,
but because the feature embedding network successfully unlearns the bias information.  
We also demonstrate quantitative and qualitative experimental results 
which show that our algorith meffectively  removes  the  bias  information  from  feature  em-bedding.

-------------
The Batch 


Bigger, Faster Transformers

Performance in language tasks rises with the size of the model — yet, as a model’s parameter count rises, 
so does the time it takes to render output. 
   >>>> New work pumps up the number of parameters without slowing down the network.
   
What’s new: William Fedus, Barret Zoph, and Noam Shazeer at Google Brain developed the Switch Transformer,
a large-scale architecture (the authors built a version comprising 1.6 trillion parameters) 
that’s nearly as fast as a much smaller model.

Key insight: The approach known as mixture-of-experts uses only a subset of a model’s parameters per input example. 
Like mixture-of-experts, Switch Transformer 
   >>> chooses which of many layers would best process a given input.

------

The Batch 

Attention for Image Generation

Attention quantifies how each part of one input affects the various parts of another. 
Researchers added a step that reverses this comparison to produce more convincing images.

What’s new: Drew A. Hudson at Stanford and C. Lawrence Zitnick at Facebook chalked up a new state of the art in generative modeling by integrating attention layers into a generative adversarial network (GAN). They call their system GANsformer.

Key insight: Typically, a GAN learns through competition between a generator that aims to produce realistic images and a discriminator that judges whether images are generated or real. StyleGAN splits the generator into (a) a mapping network and (b) a synthesis network, and uses the output of the mapping network to control high-level properties (for example, pose and facial expression) of an image generated by the synthesis network. The output of the mapping layer can be viewed as a high-level representation of the scene, and the output of each layer of the synthesis network as a low-level representation. The authors devised a two-way version of attention, which they call duplex attention, to refine each representation based on the other.

How it works: GANsformer is a modified StyleGAN. The authors trained it on four types of subject matter: faces in FFHQ; scenes composed of cubes, cylinders, and spheres in CLEVR; pictures of bedrooms in LSUN; and urban scenes in Cityscapes.

    Given a random vector, the mapping network produced an intermediate representation via a series of fully connected layers. Given a random vector, the synthesis network produced an image via alternating layers of convolution and duplex attention.
    The authors fed the mapping network's intermediate representation to the synthesis network’s first duplex attention layer.
    Duplex attention updated the synthesis network’s representation by calculating how each part of the image influenced the parts of the intermediate representation. Then it updated the intermediate representation by calculating how each of its parts influenced the parts of the image. In this way, the system refined the mapping network’s high-level view according to the synthesis network’s low-level details and vice versa.
    The discriminator used duplex attention to iteratively hone the image representation along with a learned vector representing general scene characteristics. Like the synthesis network, it comprised alternating layers of convolution and duplex attention.

Results: GANsformer outperformed the previous state of the art on CLEVR, LSUN-Bedroom, and Cityscapes (comparing Fréchet Inception Distance based on representations produced by a pretrained Inception model). For example, on Cityscapes, GANsformer achieved 5.7589 FID compared to StyleGAN2’s 8.3500 FID. GANsformer also learned more efficiently than a vanilla GAN, StyleGAN, StyleGAN2, k-GAN, and SAGAN. It required a third as many training iterations to achieve equal performance.

Why it matters: Duplex attention helps to generate scenes that make sense in terms of both the big picture and the details. Moreover, it uses memory and compute efficiently: Consumption grows linearly as input size increases. (In transformer-style self-attention, which evaluates the importance of each part of an input with respect to other parts of the same input, memory and compute cost grows quadratically with input size.)

We’re thinking: Transformers, which alternate attention and fully connected layers, perform better than other architectures in language processing. This work, which alternates attention and convolutional layers, may bring similar improvements to image processing.

