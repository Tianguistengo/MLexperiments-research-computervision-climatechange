
STANDARD DEVIATION 
is a number used to tell 
how measurements for a group are spread out from the average (mean), or expected value. 
A low standard deviation means that most of the numbers are close to the average.
A high standard deviation means that the numbers are more spread out.


------------------->  Derivatives/Math

In mathematics, the derivative is a way to --->  show rate of change:
that is,  --->  the amount by which a function is changing at_one_given_point.

For functions that act on the real numbers, 
it is --->  the slope of the tangent line at a point on a graph. 

The derivative is often written using "dy over dx" (meaning the difference in y divided by the difference in x). 

A function (black) and a tangent (red).
The derivative at the point is the slope of the tangent.




----->  Sigmoid Function 
by google glossary
(s-shaped , used by biological neurons)

A function that maps logistic or multinomial regression output (log odds) to probabilities,
returning a value between 0 and 1. 

In some neural networks, the sigmoid function acts as the >>activation function.

The sigmoid function is used for the -->  two-class logistic regression, 
whereas the softmax function is used for the -->  multiclass logistic regression

Getting to the point, the basic practical difference between Sigmoid and Softmax
is that while both give output in [0,1] range,
softmax ensures that the sum of outputs along channels (as per specified dimension) is 1 i.e., they are probabilities.
Sigmoid just makes output between 0 to 1.



----CUDA
CUDA (Computer Unified Device Architecture)
is NVIDIA's parallel computing architecture 
that enables dramatic increases in computing performance by harnessing the power of the GPU.


-
16 bit precision

 revision of IEEE 754, published in 2008, defines a floating point format that occupies only 16 bits. 
 Known as binary16, it is primarily intended to reduce storage and memory bandwidth requirements. 
 Since it provides only "half" precision, its use for actual computation is problematic.
 
  It is intended for storage of floating-point values in applications where
  >higher precision is not essential for performing arithmetic computations. 
  
  
  
  -
  What is Gradient Clipping?

Gradient clipping is a technique to >>> prevent exploding gradients.
There are many ways to compute gradient clipping,
but a common one is to rescale gradients so that their norm is at most a particular value. 

With gradient clipping, pre-determined gradient threshold be introduced,
and  then gradients norms that exceed this threshold are scaled down to match the norm.  
This prevents any gradient to have norm greater than the threshold and thus the gradients are clipped. 

There is an introduced bias in the resulting values from the gradient, 
but gradient clipping can keep things stable. 

Why is this Useful?
Training recurrent neural networks can be very difficult. 
>>Two common issues with training recurrent neural networks are vanishing gradients and exploding gradients.


-
Checkpointing

is a technique that provides fault tolerance for computing systems. 
It basically consists of saving a snapshot of the application's state,
so that applications can restart from that point in case of failure.

-
In computer science, ------>>>>>>>>Monte Carlo tree search (MCTS) 

is a heuristic search algorithm for some kinds of decision processes, 
most notably those employed in game play. 
MCTS was introduced in 2006 for computer Go.
It has been used in other board games like chess and shogi,
games with incomplete information such as bridge and poker,
as well as in turn-based-strategy video games

Monte Carlo Tree Search is a method usually used in games 
to predict the path (moves) that should be taken by the policy to reach the final winning solution.


--- 
NAIVES BAYES

why is it called naives?


---
Difference between gradients and derivatives

The gradient is a vector; it points in the direction of steepest ascent.

The directional derivative is a number; it is the rate of change when your point in â„3
moves in that direction. 

Be careful that directional derivative of a function is a scalar while gradient is a vector.

The only difference between derivative and directional derivative is the definition of those terms. Remember:

    Directional derivative is the instantaneous rate of change (which is a scalar) of ð‘“(ð‘¥,ð‘¦)
     in the direction of the unit vector ð‘¢

   Derivative is the rate of change of ð‘“(ð‘¥,ð‘¦)
   , which can be thought of the slope of the function at a point (ð‘¥0,ð‘¦0).
   
   
   -------
   
    Churchâ€“Turing thesis (also known as computability thesis)
    
     is a hypothesis about >>the nature of computable functions. 
     
     It states that a function on the natural numbers can be calculated by an effective method if and only if it is computable by a Turing machine.
   
   The Church-Turing thesis (CTT)
   underlies tantalizing open questions concerning >>the fundamental place of computing in the physical universe.
   
   For example, is every physical system computable?
   Is the universe essentially computational in nature? 
   What are the implications for computer science of recent speculation about physical uncomputability? 
   Does CTT place a fundamental logical limit on what can be computed, a computational "barrier" that cannot be broken,
   no matter how far and in what multitude of ways computers develop? 
   Or could new types of hardware, based perhaps on quantum or relativistic phenomena, lead to radically new computing paradigms 
   that do breach the Church-Turing barrier, in which the uncomputable becomes computable, in an upgraded sense of "computable"?
   
   Before addressing these questions, we first look back to the 1930s to consider how Alonzo Church and Alan Turing formulated, and sought to justify, 
   their versions of CTT. With this necessary history under our belts, 
   >we then turn to today's dramatically more powerful versions of CTT.
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
